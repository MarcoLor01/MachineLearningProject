{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf616d30-ab47-4786-83cf-25525530c60a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.load(\"pneumonia_images.npy\")\n",
    "y = np.load(\"pneumonia_labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc4677",
   "metadata": {},
   "source": [
    "Visualizziamo alcune immagini del dataset, target = 1 => Polmonite, target = 0 => Negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40809984-a9d9-443a-8729-57cc4d85c093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of images:\", X.shape[0])\n",
    "positive = 0\n",
    "negative = 0\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    if(y[i]==1):\n",
    "        positive += 1\n",
    "    if(y[i]==0):\n",
    "        negative += 1\n",
    "\n",
    "print(\"Number of positive cases:\", positive)\n",
    "print(\"Number of negative cases:\", negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da3ab4-3a2a-42f2-902d-a1d28be8bba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Visualizza alcune immagini\n",
    "num_images_to_display = 5\n",
    "\n",
    "for i in range(num_images_to_display):\n",
    "    # Seleziona un'immagine casuale\n",
    "    index = np.random.randint(0, len(X))\n",
    "    image = X[index]\n",
    "    label = y[index]\n",
    "\n",
    "    # Visualizza l'immagine con la sua etichetta\n",
    "    plt.subplot(1, num_images_to_display, i + 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33926c7d",
   "metadata": {},
   "source": [
    "Il dataset è sbilanciato verso le immagini con polmonite:\n",
    "\n",
    "Circa il 74.2% delle immagini rappresentano casi di polmonite.\n",
    "Circa il 25.8% delle immagini rappresentano casi negativi.\n",
    "\n",
    "Andiamo a suddividere il train set tra training e test, utilizzando stratify per mantenere proporzione tra le classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1916de-eae4-4bf7-a407-1e4bbeaa42da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suddivisione in set di addestramento e set di validazione\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f0917",
   "metadata": {},
   "source": [
    "A questo punto andiamo a dividere il Training Set in Training e Validation, mantenendo la stessa proporzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef254ae6",
   "metadata": {},
   "source": [
    "Vado a normalizzare i dati dopo aver suddiviso Training, Validation e Test per evitare Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_reshaped = X_valid.reshape(X_valid.shape[0], -1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "X_train_normalized = scaler.fit_transform(X_train_reshaped).reshape(X_train.shape)\n",
    "X_val_normalized = scaler.transform(X_val_reshaped).reshape(X_valid.shape)\n",
    "X_test_normalized = scaler.transform(X_test_reshaped).reshape(X_test.shape)\n",
    "\n",
    "print(f\"Normalized training set size: {X_train_normalized.shape}\")\n",
    "print(f\"Normalized validation set size: {X_val_normalized.shape}\")\n",
    "print(f\"Normalized test set size: {X_test_normalized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b296c",
   "metadata": {},
   "source": [
    "Essendo le classi sbilanciate, preferiamo una metrica come F1 rispetto a accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Funzione per calcolare l'F1-score\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411e5d67",
   "metadata": {},
   "source": [
    "Definiamo una funzione per la stampa dei vari grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57630cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graphics(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot della perdita\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot delle metriche\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.plot(history.history['f1_score'], label='Training F1 Score')\n",
    "    plt.plot(history.history['val_f1_score'], label='Validation F1 Score')\n",
    "    plt.title('Training and Validation Metrics')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5be0b-6da9-418a-b047-577217dab487",
   "metadata": {},
   "source": [
    "Andiamo a eseguire il Training tramite Convolutional NN, utilizziamo una semplice CNN composta da 2 strati convoluzionali e uno strato Dense da 128 unità per la classificazione binaria, quest'ultima realizzata con un livello Dense di 1 unità con funzione di attivazione sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12418f94-d885-4125-8489-ea04898f6c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92116088-e928-4834-9050-296603236735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn1 = Sequential([\n",
    "    Conv2D(8, kernel_size=(3, 3), activation='relu',padding='same',input_shape=(28,28,1)),\n",
    "    MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "    Conv2D(16, kernel_size=(5, 5), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "    Flatten(),\n",
    "    Dense(120, activation='relu'),\n",
    "    Dense(84, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083aba0-de45-4720-933f-fb7b17c638c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stampa una rappresentazione del modello\n",
    "cnn1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b3d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilazione del modello\n",
    "cnn1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fe812-1f11-44c1-85e6-76ea83ba5643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = cnn1.fit(X_train_normalized, y_train, epochs=50, batch_size=32, validation_data=(X_val_normalized, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29975c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred_prob = cnn1.predict(X_val_normalized)\n",
    "y_pred = np.round(y_pred_prob).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301fd694",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_normalized, y_test, batch_size=32)\n",
    "\n",
    "print(f\"Loss sul set di test: {test_loss}\")\n",
    "print(f\"Accuracy sul set di test: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e239d",
   "metadata": {},
   "source": [
    "Il modello, dalla curva riguardante la Validation Loss, mostra Overfitting, proviamo ad usare tecniche per ridurlo\n",
    "\n",
    "Tecniche usate:\n",
    "    -Dropout\n",
    "    -Early Stopping\n",
    "\n",
    "Aggiungiamo inoltre Batch Normalization.\n",
    "In questo caso andiamo a stabilire dei valori base, successivamente faremo del tuning degli hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f971236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ac1d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Definizione del modello\n",
    "cnn1 = Sequential([\n",
    "    Conv2D(8, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da92153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilazione del modello\n",
    "cnn1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_score])\n",
    "\n",
    "# Definizione del callback EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "cnn1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestramento del modello\n",
    "history2 = cnn1.fit(X_train_normalized, y_train, \n",
    "                    epochs=50, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_val_normalized, y_valid), \n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af029fdf-9164-498d-9506-1c678d74da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphics(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b26592",
   "metadata": {},
   "source": [
    "Abbiamo un buon miglioramento della curva della validation loss, mentre abbiamo valori di Validation F1 Score e Validation Accuracy leggermente fluttuanti nelle varie epoche. Vediamo la Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86602cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred_prob = cnn1.predict(X_val_normalized)\n",
    "y_pred = np.round(y_pred_prob).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, test_f1 = cnn1.evaluate(X_test_normalized, y_test, batch_size=32)\n",
    "\n",
    "print(f\"Loss sul set di test: {test_loss}\")\n",
    "print(f\"Accuracy sul set di test: {test_accuracy}\")\n",
    "print(f\"F1 Score sul set di test: {f1_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598d20f",
   "metadata": {},
   "source": [
    "Inseriamo Data Augmentation.\n",
    "Inanzitutto adattiamo i dati per avere 4 dimensioni, input richiesto per Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train_normalized, axis=-1)\n",
    "X_val = np.expand_dims(X_val_normalized, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    vertical_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=False,  # Non riflettere lungo l'asse x per immagini mediche\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Definisci il generatore di dati per il set di validazione senza data augmentation\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "# Assicurati di adattare i generatori ai tuoi dati\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "val_generator = val_datagen.flow(X_val, y_valid, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ac32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza alcune immagini originali\n",
    "num_images_to_display = 5\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i in range(num_images_to_display):\n",
    "    # Seleziona un'immagine casuale\n",
    "    index = np.random.randint(0, len(X_train))\n",
    "    original_image = X_train[index]\n",
    "    original_label = y_train[index]\n",
    "\n",
    "    # Visualizza l'immagine originale con la sua etichetta\n",
    "    plt.subplot(2, num_images_to_display, i + 1)\n",
    "    plt.imshow(original_image, cmap='gray')\n",
    "    plt.title(f\"Original Label: {original_label}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "# Visualizza alcune immagini generate\n",
    "for i in range(num_images_to_display):\n",
    "    augmented_data = next(train_generator)\n",
    "    augmented_image = augmented_data[0][0]  # Ottieni l'immagine generata\n",
    "    augmented_label = augmented_data[1][0]  # Ottieni l'etichetta generata\n",
    "\n",
    "    # Visualizza l'immagine generata con la sua etichetta\n",
    "    plt.subplot(2, num_images_to_display, num_images_to_display + i + 1)\n",
    "    plt.imshow(augmented_image, cmap='gray')\n",
    "    plt.title(f\"Augmented Label: {augmented_label}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilazione del modello\n",
    "cnn2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',f1_score])\n",
    "\n",
    "# Definizione del callback EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Addestramento del modello con data augmentation\n",
    "history = cnn2.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train_normalized) // 32,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(X_val_normalized) // 32,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611feb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ede370",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, test_f1 = cnn2.evaluate(X_test_normalized, y_test, batch_size=64)\n",
    "\n",
    "print(f\"Loss sul set di test: {test_loss}\")\n",
    "print(f\"Accuracy sul set di test: {test_accuracy}\")\n",
    "print(f\"F1 Score sul set di test: {f1_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444dae00",
   "metadata": {},
   "source": [
    "Proviamo tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74431898",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras-tuner --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6a0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
